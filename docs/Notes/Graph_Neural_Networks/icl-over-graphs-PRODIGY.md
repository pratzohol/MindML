---
title: "ICL : PRODIGY"
tags:
    - Prompting
    - ICL
    - Graphs
---

# ICL over Graphs : PRODIGY

In this note, I will cover the following paper ["PRODIGY : Enabling in-context learning over graphs"](https://arxiv.org/abs/2305.12600).

_**NOTE**_ : Definition of [[in-context-learning|In-context Learning (ICL)]] is already covered.


## 1. Abstract

- ICL is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters.
- The first pretraining framework that enables ICL over graphs - PRODIGY (**Pr**etraining **O**ver **D**iverse **I**n-Context **G**raph S**y**stems)

## 2. Introduction

- ICL : Capability of a pretrained model to perform diverse tasks directly at the prediction time when prompted with just a few examples, without any model training or fine tuning.
- **Challenges** :
    1. How to formulate node-, edge- and graph-level tasks over graphs with a unified task representation so that model performs diverse tasks without any fine-tuning or retraining.
    2. How to design model architecture and pre-training objectives that enables model to achieve ICL over graphs.
- Existing Work :
    - Pre-training just learns good graph encoder and then fine-tuning is done for each downstream task.
    - Meta-learning generalizes across tasks within the same graph.
- ICL over graph means generalizing across _graphs_ and _tasks_ without any fine-tuning or retraining.

![prodigy](../../assets/Notes/Graph_Neural_Networks/icl-over-graphs-prodigy-1.png){.center}

[//begin]: # "Autogenerated link references for markdown compatibility"
[in-context-learning|In-context Learning (ICL)]: ../Miscellaneous/in-context-learning "In-context Learning (ICL)"
[//end]: # "Autogenerated link references"


[//begin]: # "Autogenerated link references for markdown compatibility"
[in-context-learning|In-context Learning (ICL)]: ../Miscellaneous/in-context-learning "In-context Learning (ICL)"
[//end]: # "Autogenerated link references"