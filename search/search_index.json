{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>I'm a Fourth year Undergraduate at IIT Delhi majoring in Electrical Engineering. My core interests are in Machine Learning, Deep Learning, and Artificial Intelligence. In addition to these, I enjoy new technology, problem-solving and playing outdoor sports.</p>"},{"location":"#what-is-mindml","title":"What is MindML?","text":"<p>Inspired from Notes on AI, MindML is a personal wiki on AI and ML. It is a collection of notes, resources, and ideas on various topics in AI and ML. The search function and simplistic site makes it easy to navigate and revisit the concepts I have learnt.</p>"},{"location":"#why-mindml","title":"Why MindML?","text":"<p>First of all, I was bored. Second, the concepts, papers and articles I read were all lost and forgotten after few weeks. If I wanted to remember it, I had to again go through the cumbersome task of reading from various sources. </p> <p>Hence, it's better to organize everything in one place. This will also help me in revising the concepts and ideas I have learnt.</p>"},{"location":"#how-i-use-mindml","title":"How I use MindML?","text":"<p>To use MindML, I want myself to go through the pain of reading various articles only once. After that, I want to organize the concepts in my own words and in a way that I can easily understand.</p>"},{"location":"Maths/bigoplus/","title":"\\bigoplus","text":""},{"location":"Maths/bigoplus/#1-bigoplus-operator","title":"1. \\bigoplus operator","text":"<p>This operator looks like this \\(\\rightarrow\\bigoplus\\). In LaTeX, we use <code>$\\bigoplus$</code> for this, and hence, the title name.(since you cannot use LaTeX math mode in title ;-;)</p>"},{"location":"Maths/bigoplus/#2-definition","title":"2. Definition","text":"<p>\\(\\bigoplus\\) is a direct sum operator. I will give an example using graphs to explain this.</p> <ul> <li>Suppose only the nodes {\\(v_1\\), \\(v_2\\), \\(v_3\\)} \\(\\in \\mathcal{V}\\) are given and I want to construct a subgraph \\(\\mathcal{G}^D\\) = (\\(\\mathcal{V}^D\\), \\(\\mathcal{E}^D\\), \\(\\mathcal{R}^D\\)) around it by sampling its \\(k\\)-hop neighborhood from the graph \\(\\mathcal{G}\\).</li> <li>Then we find the \\(k\\)-hop neighbors of each node \\(v_m \\in \\mathcal{V}\\), denoted as (\\(\\mathcal{V}_{m}\\), \\(\\mathcal{E}_{m}\\), \\(\\mathcal{R}_{m}\\)).</li> <li>\\((\\mathcal{V}_{m}, \\mathcal{E}_{m}, \\mathcal{R}_{m}) =\\) subgraph obtained by sampling \\(k\\)-hop neighborhood of the node \\(v_m \\in \\mathcal{V} \\;\\;\\; \\forall m = 1, 2, 3\\).</li> <li>Finally, you have sets of vertices, sets of edges and sets of relations. We take union of these sets respectively over \\(m = 1, 2, 3\\) to obtain the desired subgraph \\(\\mathcal{G}^D\\).  </li> <li>\\(\\mathcal{V}^D = \\bigcup_{m=1}^{m=3} \\mathcal{V}_{m}\\), \u2002 \\(\\mathcal{E}^D = \\bigcup_{m=1}^{m=3} \\mathcal{E}_{m}\\) \u2002 and \u2002 \\(\\mathcal{R}^D = \\bigcup_{m=1}^{m=3} \\mathcal{R}_{m}\\)</li> <li>This can be equivalently represented as \\(\\mathcal{G}^D = \\bigoplus_{m=1}^{m=3} (\\mathcal{V}_{m}, \\mathcal{E}_{m}, \\mathcal{R}_{m})\\)</li> </ul>"},{"location":"Notes/list-of-papers/","title":"List of Papers Covered","text":"<p>List of all the papers I've covered till date :</p> S.no. Name of Paper Link to Note 1. GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks Click here 2. PRODIGY : Enabling In-context learning over graphs Click here 3. Talk like a Graph : Encoding Graphs for LLMs Click here 4. ONE FOR ALL: TOWARDS TRAINING ONE GRAPH MODEL FOR ALL CLASSIFICATION TASKS Click here 5. Exploring the Potential of LLMs in Learning on Graphs Click here 6. TBA Click here"},{"location":"Notes/Definitions/","title":"Definitions","text":"","tags":["Basics"]},{"location":"Notes/Definitions/#1-overview","title":"1. Overview","text":"<p>This post will contain definition of various terms that I come across while reading papers and articles. I will try to keep it updated as I learn more about new terms.</p> <p>These terms are not that complex and time consuming to understand and hence, does not deserve separate posts. So, I will keep on adding them here.</p>","tags":["Basics"]},{"location":"Notes/Definitions/#2-glossary","title":"2. Glossary","text":"","tags":["Basics"]},{"location":"Notes/Definitions/#ccs-concepts","title":"# CCS Concepts","text":"<p>\"CCS Concepts\" typically refers to the concepts or topics within the field of computer science that are relevant to the research presented in the paper. CCS stands for \"ACM Computing Classification System,\" which is a standardized classification system used in the computer science community to categorize research papers and topics.</p>","tags":["Basics"]},{"location":"Notes/Definitions/#ciyml","title":"# CI.yml","text":"<p>CI.yml is a configuration file used in continuous integration (CI) workflows. It is commonly used in conjunction with tools like GitHub Actions, GitLab CI/CD, or other CI/CD platforms. The \"ci.yml\" file contains instructions and settings for automating the build, test, and deployment processes of a software project. It defines the steps, dependencies, environment variables, and other parameters required to perform automated testing and integration tasks.</p>","tags":["Basics"]},{"location":"Notes/Definitions/#pre-train-fine-tune","title":"# Pre-train, Fine-tune","text":"<p>The pre-train, fine-tune paradigm is a common approach used in transfer learning, particularly in the context of deep learning models. It involves two main stages:</p> <ol> <li>Pre-training : In the pre-training stage, a neural network model is trained on a large and diverse dataset, typically on a related but different task. This initial training is called \"pre-training\" because the model is not yet specialized for the specific downstream task of interest. Instead, it learns general features and representations from the data in an unsupervised or supervised manner.</li> <li>Fine-tune :  In this stage, the pre-trained model is further trained on a smaller, task-specific dataset. During fine-tuning, the earlier layers of the model, which capture more generic features, are usually frozen or updated with a lower learning rate to preserve the general knowledge learned during pre-training. The later layers, which capture more task-specific information, are updated with a higher learning rate to allow the model to specialize for the specific task.</li> </ol> <p>CONS : There might be a large training gap between the pre-trained model and the downstream task. This can lead to a phenomenon called \"catastrophic forgetting,\" where the model forgets the general knowledge learned during pre-training as it learns the task-specific information during fine-tuning. This also leads to large fine-tuning time which might become comparable to the training time and hence, becomes computationally expensive.</p>","tags":["Basics"]},{"location":"Notes/Definitions/#secure-multiparty-computation-smc","title":"# Secure Multiparty Computation (SMC)","text":"<p>SMC allows a number of mutually distrustful parties to carry out a joint computation of a function of their inputs, while preserving the privacy of the inputs.</p>","tags":["Basics"]},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/","title":"Prompting over Graphs : GPPT","text":"<p>In this note, I will cover the following paper GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks</p> <p>NOTE : Definition of In-context Learning (ICL) is already covered. First, I will cover GPPT which is based on Graph prompts. Finally, moving ahead from prompting, we will see how ICL : PRODIGY introduces ICL over graphs.</p>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#1-abstract","title":"1. Abstract","text":""},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#11-motivation","title":"1.1 Motivation","text":"<ul> <li>The supervised training of GNNs notoriously requires large amounts of labeled data for each downstream task.</li> <li>Alternative : Use Transfer Learning -  using easily accessible information to pre-train GNNs, and fine-tuning them to optimize the downstream task with only a few labels.</li> <li>Design the self-supervised pretext tasks, and encode the universal graph knowledge among the various applications.</li> <li>In rare cases, there is inherent training objective gap between the pretext and downstream tasks.</li> <li>This may lead to costly fine-tuning process to adapt the pre-trained GNNs to the downstream tasks.</li> <li>The naive pre-training strategy usually deteriorates the downstream task and reduces the reliability of transfer learning on graph.</li> <li>Long fine-tuning leads to forgetting of the pre-trained general knowledge. (Catastrophic Forgetting)</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#12-contributions-of-this-paper","title":"1.2 Contributions of this Paper","text":"<ul> <li>Novel transfer learning paradigm to generalize GNNs, namely graph pre-training and prompt tuning (GPPT).</li> <li>Masked Edge Prediction is used as pretext task to pre-train GNNs.</li> <li>Node Prediction \u2705, Link Prediction \u274c, and Graph Classification \u274c.</li> <li>Graph Prompting function reformulates the downstream task by modifying the standalone node into a token pair and makes it similar to the pre-training task, i.e, edge prediction.</li> <li>The token pair consists of node entity (structure token) and candidate label class (task token).</li> <li>Instead of \"pre-train, fine-tune\" paradigm, GPPT uses \"pre-train, prompt, fine-tune\" paradigm.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#2-introduction","title":"2. Introduction","text":"<ul> <li>Task Token : They are the node labels represented by the trainable continuous vectors. It is appended to the the embedding of the target sample. The linking probability between task token and target sample is directly measured through using the pre-trained model.</li> </ul> <pre><code>The orthogonal prompt initialization and regularization are proposed to separate the trainable vectors of different labels.\n</code></pre> <ul> <li>Structure Token : It represents the target sample with its multi-hop neighbourhood.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#3-related-work-graph-pre-training","title":"3. Related Work (Graph Pre-training)","text":"<ul> <li>GCC : Leverages contrastive learning to capture the universal network topological properties across multiple networks.</li> <li>GPT-GNN : Introduces a self-supervised attributed graph generation task to pre-train GNN models that can capture the structural and semantic properties of the graph.</li> <li>L2P-GNN : Utilizes meta-learning to learn the fine-tune strategy during the pre-training process.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#4-preliminary-knowledge","title":"4. Preliminary Knowledge","text":""},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#41-gnn","title":"4.1 GNN","text":"<ul> <li>Let tuple \\(\\mathcal{G} = (X, A)\\) denote undirected graph, where \\(X \\in  \\mathbb{R}^{N \\times d}\\) is node feature matrix and \\(A \\in \\mathbb{R}^{N \\times N}\\) is adjacency matrix. </li> <li>Suppose the number of graph convolutional layers is \\(K\\). To facilitate the following expression, we use \\(h_i = f_\\theta (\\mathcal{G}, v_i)\\) to represent the final node representation learned from \\(K\\)-layer GNNs, where \\(\\theta = \\{\\theta^{(1)}, \\theta^{(2)}, \\cdots , \\theta^{(K)}\\}\\) denotes the concatenated trainable parameters.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#42-pre-train-and-fine-tune-gnns","title":"4.2 Pre-train and Fine-tune GNNs","text":"<p>The edge prediction pretext task works as follows - </p> <ul> <li>Randomly mask partial edges and then train GNNs to reconstruct them.</li> <li>Formally, let \\(\\mathcal{G}^{pre} = (X, A^{pre})\\) denote masked graph. Node Embedding of \\(v_i\\) is given by  \\(h_i = f_\\theta (\\mathcal{G}^{pre}, v_i)\\).</li> <li> <p>Pretext task is to determine whether a node pair is connected, where loss is defined as follows :</p> \\[\\min_{\\theta, \\phi} \\sum_{(v_i, v_j)} \\mathcal{L}^{pre} (p_{\\phi}^{pre}(h_i, h_j); g(v_i, v_j))\\] <p>Node pair \\((v_i, v_j)\\) is either masked edge or sampled negative unconnected pairs. \\(p_{\\phi}^{pre}\\) is projection head to evaluate similarity score of node pair. \\(g(v_i, v_j)\\) is ground truth label of the node pair (i.e., \\(A_{ij}\\)).</p> </li> </ul> <p>Now, under fine-tuning, following loss is optimized -</p> \\[\\min_{\\theta, \\psi} \\sum_{v_i} \\mathcal{L}^{down}(p_{\\psi}^{down}(h_i); g(v_i))\\] <ul> <li>\\(\\theta_{init} = \\theta_{pre}\\), i.e., GNNs are initialized by the optimized parameters from pretext task.</li> <li>\\(p^{down}_{\\phi}(h_i)\\) denotes the new projection head accompanied with parameters \\(\\phi\\), while the pretext projection head is discarded. </li> <li>\\(L^{down}\\) is the downstream loss function (e.g., cross entropy), and \\(g(v_i)\\) denotes the ground-truth label of node \\(v_i\\).</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#5-graph-prompting-framework","title":"5. Graph Prompting Framework","text":"<ul> <li>First, we note that GNNs\u2019 parameters \\(\\theta\\) are optimized to generate close embeddings for connected node pairs, instead of nodes of the same class. If the disconnected pairs share the same class, the pretrained model requires to be tuned with many epochs to adapted to the new problem. </li> <li>This time-consuming fine-tuning prevents us from efficiently using the pre-trained model. The pre-trained knowledge will also be gradually filtered out in the long tuning process.</li> <li>Second, in the initial stage of fine-tuning, the pre-trained model is not well adapted to the downstream task, and gives wrong classifications.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#51-how-to-use-prompt","title":"5.1 How to use Prompt?","text":"<ul> <li>For \\(v_i\\), generate prompt \\(v_i^{'} = f_{prompt}(v_i) = [T_{task}(y), T_{srt}(v_i)]\\) = [task token, structure token].</li> <li>Given the token pairs, \\([T_{task}(y), T_{srt}(v_i)]\\), by embedding them into continuous tensors, one is able to conduct the classification task by fitting the linking probability between the two tokens.</li> </ul> <p>GPPT consists of 3 componets --&gt;</p> <ol> <li>Prompt Addition : The graph prompting function generates a series of token pairs to be classified. Assuming that there are total \\(C\\) classes \\([y_1, y_2, \\cdots, y_C]\\), we construct their corresponding token pairs \\([T_{task}(y_c), T_{srt}(v_i)]\\), for \\(c = 1, 2, \\cdots, C\\).</li> <li>Prompt Answer : Given each token pair \\([T_{task}(y_c), T_{srt}(v_i)]\\), we embed them into continuous vectors. We then concatenate them as input to the pre-trained projection head, and obtain the linking probability. We answer and classify target node \\(v_i\\) with label \\(y_c\\) if it obtains the highest probability.</li> <li> <p>Prompt Tuning : Following the pretext training objective, following loss is optimized :</p> \\[\\min_{\\theta, \\phi} \\sum_{(v_i, y_c)} \\mathcal{L}^{pre} (p_{\\phi}^{pre} (T_{task}(y_c), T_{srt}(v_i)); g(y_c, v_i))\\] <ul> <li>\\(p_{\\phi}^{pre}\\) is the pre-training projection head to evaluate the linking probability of the token pair.</li> <li>\\(g(y_c, v_i)\\) denotes the ground truth connection between label class \\(y_c\\) and target node \\(v_i\\).</li> </ul> </li> </ol>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#52-prompt-function-design","title":"5.2 Prompt Function Design","text":""},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#521-task-token-generation","title":"5.2.1 Task Token Generation","text":"<ul> <li>The task token \\(T_{task} (y_c)\\) is embedded into a trainable vector: \\(e_c = T_{task} (y_c) \\in \\mathbb{R}^d\\).</li> <li>For the total \\(C\\) classes in the downstream node classification, the task token embeddings are defined by: \\(E = [e_1, \\cdots, e_C] \\in \\mathbb{R}^{C \\times d}\\).</li> <li>Task token is basically class-prototype node added to the original graph. </li> <li>NOTE : The optimal embedding of task token \\(T_{task} (y_c)\\) should be at the center of node embeddings of the class \\(y_c\\).</li> </ul> <p>Importance of cluster structure in graphs --&gt;</p> <ul> <li>It would be hard for the distinct nodes over graph to use and tune the single task token embeddings \\(E\\).</li> <li>Given the edge prediction pretext task, the pre-trained node embeddings will also be clustered in the embedding space.</li> <li>The optimal embedding of task token \\(T_{task} (y_c)\\) should thus vary with clusters. </li> <li>To do better node classification at each cluster, do the cluster-based task token generation, consisting of 3 steps:<ol> <li>Use scalable clustering module (e.g., METIS) to split nodes into non-overlapping clusters : \\(\\{\\mathcal{G}_1, \\cdots, \\mathcal{G}_M\\}\\) where \\(M\\) is hyper-parameter of cluster number.</li> <li>For each individual cluster \\(m\\), there will be an independent task token embeddings : \\(E^m = [e^m_1, \\cdots, e^m_C] \\in \\mathbb{R}^{C \\times d}\\).</li> <li><code>NOTE:</code> For task token \\(T_{task}(y_c)\\) for node \\(v_i\\) belonging to cluster \\(m\\), it's embedding is : \\(e_c^m\\).</li> </ol> </li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#522-structure-token-generation","title":"5.2.2 Structure Token Generation","text":"<ul> <li>According to social network theory, proximal nodes tend to possess similar features and node labels.</li> <li>It also provides redundant information which makes the classification decision robust.</li> <li>Structure token \\(T_{str} (v_i)\\) denotes subgraph centered at node \\(v_i\\).</li> <li>Used 1-hop neighbourhood only.</li> <li>The structure token \\(T_{str} (v_i)\\) is embedded into a continuos vector as :</li> </ul> \\[e_{v_i} = T_{str} (v_i) = a_i * h_i + \\sum_{v_j \\in \\mathcal{N}(v_i)} a_{j} * h_j\\] <ul> <li>\\(a_i\\) are the attention weights learned from the attention function.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#53-prompt-initialization-and-orthogonal-prompt-constraint","title":"5.3 Prompt Initialization and Orthogonal Prompt Constraint","text":"<p>Prompt Initialization --&gt;</p> <ul> <li>Optimal node embeddings should be at center of node embeddings. Random initialization may deteriorate the classification at initial stage.</li> <li>Thus, for each cluster \\(m\\), task token \\(e_c^m\\) is initialized as the mean of node embeddings of training nodes of class \\(y_c\\) in cluster \\(m\\).</li> <li>This means initialization provides the valid task tokens, and ensures the correct classification at the initial stage.</li> </ul> <p>Orthogonal Prompt Constraint --&gt;</p> <ul> <li>NOTE : To conduct correct node classification, the task token embeddings of different classes should be irrelevant to each other.</li> <li>Thus, orthogonality of task token is enforced during model fine-tuning. </li> </ul> \\[\\mathcal{L_o} = \\sum_{m} || E^m (E^m)^T - I ||_F^2\\]"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#54-overall-learning-process","title":"5.4 Overall Learning Process","text":"<ol> <li>Pre-training : Done using masked edge prediction.</li> <li>Prompt Addition : Modifies the target node \\(v_i\\) as token pair \\([T_{task}(y_c), T_{str}(v_i)] = [e_c^m, e_{v_i}]\\).</li> <li>Prompt Answer : Evaluates the class of node by assigning the task token which has highest linking probability. NOTE: The linking probability is evaluated by the pre-trained projection head.</li> <li> <p>Prompt Tuning (or Fine Tuning) :  Optimizes the GNNs and token embeddings as follows -</p> \\[\\min_{\\theta, \\phi, E^1, \\cdots, E^M} \\sum_{(v_y, y_c)} \\mathcal{L}^{pre} (p_{\\phi}^{pre} (e_c^m, e_{v_i}); g(y_c, v_i)) + \\lambda \\mathcal{L}_o\\] <ul> <li>\\(\\theta^{init} = \\theta^{pre}\\), and \\(\\phi^{init} = \\phi^{pre}\\).</li> <li>\\(\\lambda\\) is the loss hyper-parameter.</li> <li>\\(\\phi^{pre}\\) is the pre-trained parameters of the projection head.</li> </ul> </li> </ol>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#55-pseudo-algorithm","title":"5.5 Pseudo-Algorithm","text":""},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#6-experimental-setup","title":"6. Experimental Setup","text":"<ol> <li>Datasets : These datasets are available in PyG or DGL library. </li> <li>Batch Size : The batch sizes are 256 - Cora, 256 - Citeseer, 2048 - CoraFull, 256 - Pubmed, 4096 - Ogbn-arxiv, 2048 - AmazonCoBuyComputer, 4096 - AmazonCoBuyPhoto, and 4096 - Reddit.</li> <li> <ul> <li>Number of layers : 2 and Hidden units of 128 for GNNs  </li> <li>Optimizer : Adam  </li> <li>Learning Rate : [0.001, 0.005]  </li> <li>Weight Decay : 0.0005  </li> <li>Loss Hyper-parameter : \\(\\lambda = 0.01\\) </li> <li>Clustering Number : \\(M =\\) 7 (Cora), 6 (Citeseer), 3 (Corafull), 5 (Pubmed), 10 (Ogbn-arxiv), 8 (AmazonCoBuyComputer), 10 (AmazonCobuyPhoto), 10 (Reddit) </li> </ul> </li> </ol>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#61-github-code","title":"6.1 Github Code","text":"<ul> <li>Offficial github code for this paper is at GPPT</li> <li>Working code implemented by me is available at GPPT-working</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/1-icl-over-graphs-GPPT/#7-doubts","title":"7. Doubts","text":"<ol> <li>How is \\(a_i\\) weights of attention function learned in structure token generation? </li> <li>How does the orthogonal constraint equation enforces orthogonality of task token embeddings?</li> </ol>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/","title":"ICL over Graphs : PRODIGY","text":"<p>In this note, I will cover the following paper \"PRODIGY : Enabling in-context learning over graphs\".</p> <p>NOTE : Definition of In-context Learning (ICL) is already covered.</p>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#1-abstract","title":"1. Abstract","text":"<ul> <li>ICL is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters.</li> <li>The first pretraining framework that enables ICL over graphs - PRODIGY (Pretraining Over Diverse In-Context Graph Systems)</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#2-introduction","title":"2. Introduction","text":"<ul> <li>ICL : Capability of a pretrained model to perform diverse tasks directly at the prediction time when prompted with just a few examples, without any model training or fine tuning.</li> <li>Challenges :<ol> <li>How to formulate node-, edge- and graph-level tasks over graphs with a unified task representation so that model performs diverse tasks without any fine-tuning or retraining.</li> <li>How to design model architecture and pre-training objectives that enables model to achieve ICL over graphs.</li> </ol> </li> <li>Existing Work :<ul> <li>Pre-training just learns good graph encoder and then fine-tuning is done for each downstream task.</li> <li>Meta-learning generalizes across tasks within the same graph.</li> </ul> </li> <li>ICL over graph means generalizing across graphs and tasks without any fine-tuning or retraining.</li> </ul> <ul> <li>PRODIGY :  <ol> <li>It proposes prompt graph which unifies node-, edge- and graph-level tasks over graphs.</li> <li>For pre-training, it uses in-context pretraining objectives (Neighbor Matching &amp; Multi Task) which is a self-supervised task.</li> <li>It also defines a GNN structure and an attention mechanism to communicate over prompt graph.</li> </ol> </li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#3-few-shot-prompting","title":"3. Few-shot Prompting","text":"<ul> <li>We define a graph as \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E}, \\mathcal{R})\\), where \\(\\mathcal{V},\\mathcal{E}, \\mathcal{R}\\) represent the set of nodes, edges and relations. An edge \\(e = (u,r,v) \\in \\mathcal{E}\\) consists of a subject \\(u \\in \\mathcal{V}\\), a relation \\(\\mathcal{r} \\in \\mathcal{R}\\) and an object \\(v \\in \\mathcal{V}\\).</li> <li>Suppose, we have a m-way classification task with |\\(\\mathcal{Y}\\)| = m classes and we define k-shot prompt.</li> <li>prompt examples : \\(\\mathcal{S} = \\{(x_i, y_i)\\}_{i=1}^{m \\cdot k}\\) with k examples per class \\(y \\in \\mathcal{Y}\\).</li> <li>query set : \\(\\mathcal{Q} = \\{x_i\\}_{i=1}^n\\) for which we want to predict the labels for.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#4-prompt-graph","title":"4. Prompt Graph","text":""},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#41-data-graph","title":"4.1. Data Graph","text":"<ul> <li>To generate data graph \\(\\mathcal{G}^D\\), sample the k-hop neighbour of the input node set from the source graph \\(\\mathcal{G}\\).</li> <li>For node classification, the input node set is a singleton set of target node. For link prediction, it is a pair of nodes.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#42-task-graph","title":"4.2. Task Graph","text":"<ul> <li>Task graph \\(\\mathcal{G}^T\\) consists of data nodes (\\(v_{x_i}\\)) and label nodes (\\(v_{y_i}\\)).</li> <li>The data graph \\(\\mathcal{G}_i^D\\) is aggregated into a single node \\(v_{x_i}\\) and the label \\(y_i\\) is represented by a label node \\(v_{y_i}\\).</li> <li>So, a task graph contains (mk + n) data nodes and m label nodes.</li> <li>For query set, we add single directional edge from label node to query data node. </li> <li>NOTE : This is done to avoid information leakage from query set to prompt set.</li> <li>For prompt examples, we add bi-directional edge between label node and data node. The edge with true labels are marked \\(T\\), others are marked \\(F\\).</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#5-pre-training-message-passing-architecture","title":"5. Pre-training : Message Passing Architecture","text":""},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#51-data-graph-message-passing","title":"5.1. Data Graph Message Passing","text":"<ul> <li>Used GraphSAGE for data graph \\(\\mathcal{G}^D\\) to  learn embeddings \\(E\\) for each node. \\(E \\in \\mathcal{R}^{|\\mathcal{V^D}| \\text{ x d}}\\)</li> <li>For node prediction : \\(E_{v_{x_i}}\\) = \\(E_{v_i}\\) (take the node embedding of node \\(v_i\\))</li> <li>For link prediction : \\(E_{v_{x_i}}\\) = \\(W^T(E_{v_{i1}} || E_{v_{i2}} || max(E_j \\:\\:\\forall_{j \\in \\mathcal{G}^D})) + b\\) <ul> <li>concatenate the node embeddings of two nodes with max-pooling of all the node embeddings in the data graph</li> <li>Then project it back to \\(d\\)-dimensional embedding space</li> <li>\\(W \\in \\mathcal{R}^{\\text{3d x d}}\\) is a learnable weight matrix and \\(b \\in \\mathcal{R}^d\\) is a learnable bias vector.</li> </ul> </li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#52-task-graph-message-passing","title":"5.2. Task Graph Message Passing","text":"<ul> <li>The embedding of label node \\(v_{y_i}\\) can either be initialized with random Gaussian or additional information available about the labels.</li> <li>Each edge also has two binary features \\(e_{ij}\\) that indicate<ol> <li>whether the edge comes from an example or a query, and </li> <li>the edge type of \\(T\\) or \\(F\\).</li> </ol> </li> <li>The GNN architecture uses attention mechanism using \\(K\\), \\(Q\\), \\(V\\) values and is similar to a transformer.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#53-prediction-read-out","title":"5.3. Prediction Read-out","text":"<ul> <li>Take classfication logits \\(O_i\\) by taking cosine similarity between query and label node embedings.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#6-pre-training-in-context-pretraining-objectives","title":"6. Pre-training : In-context pretraining objectives","text":""},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#61-neighbor-matching","title":"6.1. Neighbor Matching","text":"<ul> <li>This is Self-supervised task.</li> <li>We sample multiple subgraphs from \\(\\mathcal{G}_{\\text{pretrain}}\\) as local neighbourhood and we say a node belongs to the neighbourhood if it is in the sampled subgraph. </li> <li>\\(\\mathtt{NM}_{k, m}\\) is sampler which generates \\(m\\)-way neighbor matching with \\(k\\)-shot prompt examples and labels of the queries.</li> <li>First, sample \\(m\\) nodes from \\(\\mathcal{G}_{\\text{pretrain}}\\) and each node corresponds to one class.</li> <li>Then, sample \\(k\\) nodes from \\(l\\)-hop neighbourhood of each class node \\(c_i\\). </li> <li>Lastly, sample \\(\\lceil \\frac{n}{m} \\rceil\\) nodes from each class node \\(c_i\\). This will be the query set.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#62-multi-task","title":"6.2. Multi-task","text":"<ul> <li>This is Supervised task.</li> <li>If node/edge level labels are present for \\(\\mathcal{G}_{\\text{pretrain}}\\), then we can leverage them to construct pretraining task similar to neighbor matching.</li> <li>Sample \\(m\\) labels from whole label set. Then sample \\(k\\) support exampls for each label class.</li> <li>Lastly, sample \\(\\lceil \\frac{n}{m} \\rceil\\) nodes from each label class to form the query set.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#63-prompt-graph-augmentation-and-pretraining-loss","title":"6.3. Prompt Graph Augmentation and Pretraining Loss","text":"<ul> <li>Prompt Graph Augmentation :<ol> <li>Node Dropping : From the data graph of samples and queries, randomly drop few nodes from its \\(l\\)-hop neighborhood.</li> <li>Node Feature Masking : From the data graph of samples and queries, mask the features as zero vector for few nodes randomly.</li> </ol> </li> <li>Pretraining Loss :<ul> <li>\\(\\mathcal{L} = \\underset{x_i \\in \\mathcal{Q}_{\\mathtt{task}}}{\\mathbb{E}} \\mathtt{CE}(O_{\\mathtt{task, i}}, y_{\\mathtt{task, i}})\\) where \\(\\mathtt{task}\\) = \\(\\mathtt{NM}\\) and \\(\\mathtt{MT}\\). </li> <li>This is cross-entropy loss between logits and true class label.</li> </ul> </li> </ul>"},{"location":"Notes/Graph_Neural_Networks/2-icl-over-graphs-PRODIGY/#7-experiments","title":"7. Experiments","text":""},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/","title":"ICL over Graphs : OFA","text":"<p>In this note, I will cover the following paper \"ONE FOR ALL: TOWARDS TRAINING ONE GRAPH MODEL FOR ALL CLASSIFICATION TASKS\". This paper benchmarks against ICL : PRODIGY. </p> <p>NOTE : Definition of In-context Learning (ICL) is already covered.</p>"},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#1-overview","title":"1. Overview","text":"<ol> <li>LLMs are used to encode node, edge, graph level information. These are called Text-attributed graphs (TAGs).<ul> <li>OFA uses TAGs to integrate graph datasets from different domains into one large TAG dataset and leverages the power of LLMs to learn from all domains jointly.</li> <li>All nodes and edges in the graphs are described using human-readable texts and embedded from different domains into the same embedding space with a single LLM.</li> </ul> </li> <li>OFA introduces the concept of nodes-of-interest (NOI) to standardize different tasks with a single task representation.<ul> <li>NOI subgraph and NOI prompt node unifies different types of tasks.</li> <li>It improves the ability of fondation model to learn structural information.</li> </ul> </li> <li>For ICL on graphs,  OFA introduces a novel graph prompting paradigm (GPP) which enables it to address various tasks without fine-tuning.<ul> <li>It inserts prompt graph into original graph in a task-specific way.</li> <li>The nodes in the prompt graph contain all related information about the downstream task. </li> <li>Then, the modified graph becomes the actual input to the foundation model.</li> </ul> </li> </ol>"},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#11-comparison-with-prodigy","title":"1.1. Comparison with PRODIGY","text":"<ul> <li>OFA utilizes LLMS to encode node, edge and graph level information using text prompts whereas PRODIGY just encodes the textual feature of nodes.</li> <li>OFA supports zero-shot prompting whereas PRODIGY only supports few-shot prompting.</li> <li>Cross domain graphs cannot be handled by PRODIGY whereas OFA encodes cross domain graphs using LLMs.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#2-one-for-all-foundation-model","title":"2. One-For-All : Foundation Model","text":""},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#21-text-attributed-graphs-tags","title":"2.1. Text-attributed Graphs (TAGs)","text":"<ul> <li>For node feature, \\(s_{v_i}\\) = <code>Feature node. &lt;f desc&gt;:&lt;f content&gt; ; &lt;f desc&gt;:&lt;f content&gt; ; ...</code> prompt is used where <code>&lt;f desc&gt;</code> is the description of the feature and <code>&lt;f content&gt;</code> is the content of the feature.</li> <li>Similary for edge feature \\(s_{e_{ij}}\\), they use <code>Feature edge</code> in place of <code>Feature node</code>.</li> <li>For node \\(v_i\\) and edge \\(e_{ij}\\), their vector representations are \\(x_i\\) = LLM(\\(s_{v_i}\\)) and \\(x_{ij}\\) = LLM(\\(s_{e_{ij}}\\)) respectively.</li> <li>In OFA, they used SentenceTransformer to encode the text into vectors.</li> <li>LLMs capture the domain information of features which is passed down to the foundation model.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#22-nodes-of-interest-noi","title":"2.2. Nodes-of-Interest (NOI)","text":"<ul> <li>NOI refers to the blue nodes in the above figure. It is single node for node classification and pair of nodes for link prediction.</li> <li>NOI subgraph is the \\(h\\)-hop neighborhood of NOI node(s).</li> <li>NOI prompt node connects to all the nodes in NOI.<ul> <li>It has the task prompt <code>Prompt node. &lt;task desc&gt;</code>.</li> <li>Through message passing, NOI prompt node summarizes information in NOI and task description.</li> <li>NOTE : Other works utilize the subgraph and then pool the information to unify different tasks. In contrast, OFA uses NOI prompt node to unify different tasks and thus eliminates the need for explicit pooling and instead, directly uses message passing.</li> </ul> </li> </ul>"},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#23-graph-prompting-paradigm-gpp-for-graph-icl","title":"2.3. Graph Prompting Paradigm (GPP) for Graph ICL","text":"<ul> <li>Prompt Graph consists of 2 types of nodes : 1) NOI prompt node \\(p_q\\) and, 2) class nodes \\(c_i\\).</li> <li>Class node \\(c_i\\) has the prompt <code>Prompt node. &lt;class desc&gt;</code>.</li> <li>Relation type of edges from NOI to NOI prompt node is \\(r_{t2p}\\) and \\(r_{p2t}\\) for reverse edge.</li> <li>Relation type of edges from NOI prompt node to class node is \\(r_{q2c}\\) and \\(r_{c2q}\\) for reverse edge.</li> <li>Relation type of edges from NOI prompy node of support examples to class node is \\(r_{s2c}\\) and \\(r_{c2s}\\) for reverse edge.</li> <li>Since \\(r_{s2c}\\) and \\(r_{q2c}\\) differs, model can differentiate between query and support examples.</li> <li>Prompted graph, \\(\\mathcal{G}_m\\) is the NOI subgraph + prompt graph. This prompted graph is input to the subsequent graph model.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#24-prediction","title":"2.4. Prediction","text":"<ul> <li>Let \\(h_{c_i}\\) be the vector representation of class node \\(c_i\\) after the graph model is applied.</li> <li>\\(\\mathbb{P}\\text{[NOI belongs to class i]} = \\sigma(\\text{MLP}(h_{c_i}))\\). Take \\(\\text{argmax}\\) to get the class label.</li> <li>\\(h_{c_i}\\) contains class description, NOI information and task information and hence, prediction is task dependent.</li> <li>GNN capable of recognizing different edge types serves as graph model in OFA.</li> </ul>"},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#3-related-work","title":"3. Related Work","text":""},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#4-implementation-of-ofa","title":"4. Implementation of OFA","text":""},{"location":"Notes/Graph_Neural_Networks/3-one-for-all-ofa/#5-experiments","title":"5. Experiments","text":""},{"location":"Notes/Miscellaneous/contrastive-learning/","title":"Contrastive Learning","text":""},{"location":"Notes/Miscellaneous/contrastive-learning/#1-definition","title":"1. Definition","text":"<p>Contrastive learning aims at learning low-dimensional representations of data by contrasting between similar and dissimilar samples.</p> <p>What this means is that it tries to bring similar samples close to each other in the representation space and push dissimilar samples away from each other.</p> <p>Let's suppose we have 3 images, \\(I_1\\), \\(I_2\\) and \\(I_3\\), where \\(I_1\\) and \\(I_2\\) belongs to same class (e.x. dog) and \\(I_3\\) belongs to different class (e.x. cat). The representation space will look something like this:</p> <p></p> <p>We see that the distance \\(d(x_1, x_2)\\) is small compared to \\(d(x_1, x_3)\\) and \\(d(x_2, x_3)\\) where \\(d()\\) is a metric function like euclidean.</p>"},{"location":"Notes/Miscellaneous/contrastive-learning/#2-loss-functions","title":"2. Loss Functions","text":""},{"location":"Notes/Miscellaneous/contrastive-learning/#21-contrastive-loss","title":"2.1. Contrastive Loss","text":"<p>We suppose that we have a pair (\\(I_i\\), \\(I_j\\)) and a label \\(Y\\) that is equal to 0 if the samples are similar and 1 otherwise. To extract a low-dimensional representation of each sample, we use a Convolutional Neural Network \\(f\\) that encodes the input images \\(I_i\\) and \\(I_j\\) into an embedding space where \\(x_i = f(I_i)\\) and \\(x_j = f(I_j)\\). The contrastive loss is defined as:  </p> \\[\\mathbf{L = (1-Y) * ||x_i - x_j||^2 + Y * max(0, m - ||x_i - x_j||^2)}\\] <p>where \\(m\\) is a hyperparameter, defining lower bound distance between dissimilar samples. This can be thought of as :  </p> <ul> <li>If \\(Y = 0\\), then samples are similar and hence, we want to minimize the distance between \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\).  </li> <li>If \\(Y = 1\\), then samples are dissimilar and minimizing \\(\\mathbf{L}\\) means we want to make the term \\(m - ||x_i - x_j||^2\\) \\(\\leq\\) 0. Thus, we want the distance between \\(\\mathbf{x_i}\\) and \\(\\mathbf{x_j}\\) to be maximized and greater than the lower bound \\(m\\).</li> <li>[NOTE :] This is not a classification task. We are trying to learn the embedding \\(\\mathbf{x_i}\\) for the \\(i\\)-th sample.</li> </ul>"},{"location":"Notes/Miscellaneous/contrastive-learning/#22-triplet-loss","title":"2.2. Triplet Loss","text":"<p>It takes triplet as input : an anchor sample \\(I\\), a positive sample \\(I^{+}\\) and a negative sample \\(I^{-}\\). During training, the loss function enforces the distance between anchor and positive sample to be less than the distance between anchor and negative sample.</p> <p>The triplet loss is defined as:</p> \\[\\mathbf{L = max(0, ||x - x^{+}||^2 - ||x - x^{-}||^2 + m)}\\] <p>where \\(m\\) is a hyperparameter defined in contrastive loss. Here also, we want dissimilar to be far and similar to be close so that the term \\(||x - x^{+}||^2 - ||x - x^{-}||^2 + m\\) \\(\\leq\\) 0.</p>"},{"location":"Notes/Miscellaneous/contrastive-learning/#3-types-of-learning","title":"3. Types of Learning","text":""},{"location":"Notes/Miscellaneous/contrastive-learning/#31-self-supervised-learning","title":"3.1. Self-Supervised Learning","text":"<p>When we don't have labeled samples, we use unsupervised learning, also known as self-supervised learning.</p> <p>A famous self-supervised framework for unsupervised contrastive learning is SimCLR. Its main idea is to generate positive image pairs by applying random transformations in the anchor image like crop, flip and color jitter since these changes keep the label of the image unchanged.</p>"},{"location":"Notes/Miscellaneous/contrastive-learning/#32-supervised-learning","title":"3.2. Supervised Learning","text":"<p>The label of each sample is available during training. So, we can generate positive and negative pairs or triplets by just looking at the labels. </p> <p>Positive Pairs : Samples with same label Negative Pairs : Samples with different label</p> <p>However, generating all possible pairs or triplets requires a lot of time and computational resources. Also, in every dataset, there are many negative pairs or triplets that already satisfy the contrastive training objectives and give zero loss resulting in slow training convergence.</p> <p>To deal with this problem, we have to generate hard pairs and hard triplets, meaning that their loss value is high, i.e., similar pairs that are far apart and dissimilar pairs that are very close. </p> <p>Many hard negative mining methods have been proposed that usually look into the representation space for hard pairs and triplets using fast search algorithms.</p> <pre><code>It only makes sense that this kind of contrastive learning will be used mostly with self-supervised (unsupervised) or semi-supervised settings.\n</code></pre>"},{"location":"Notes/Miscellaneous/contrastive-learning/#4-related-discussion","title":"4. Related Discussion","text":"<ol> <li>Different types of Learning in Machine Learning setting is covered in this post Types of Learning.</li> </ol>"},{"location":"Notes/Miscellaneous/erdos-renyl-model/","title":"Erdos Renyl Model : Generating Random Graphs","text":""},{"location":"Notes/Miscellaneous/erdos-renyl-model/#1-overview","title":"1. Overview","text":"<ul> <li> <p>There are two variants of Erdos Renyl Model:</p> <ul> <li>G(n, p) : n nodes, each edge is present with probability p</li> <li>G(n, m) : n nodes, m edges are chosen uniformly at random from the set of all possible edges</li> </ul> </li> <li> <p>Equivalently, all graphs with n nodes and M edges have equal probability of \\(p^M (1-p)^{\\binom{n}{2}-M}\\)</p> </li> <li>The parameter p in this model can be thought of as a weighting function; as p increases from 0 to 1, the model becomes more and more likely to include graphs with more edges and less and less likely to include graphs with fewer edges.</li> <li>A graph in G(n, p) has on average \\({\\binom{n}{2}}p\\) edges. </li> <li>The distribution of the degree of any particular vertex is binomial: \\(P(deg(v)=k)=\\binom{n-1}{k}{p^k}(1-p)^{n-1-k}\\) </li> </ul>"},{"location":"Notes/Miscellaneous/erdos-renyl-model/#2-references","title":"2. References","text":"<ul> <li>https://www.geeksforgeeks.org/erdos-renyl-model-generating-random-graphs/ </li> </ul>"},{"location":"Notes/Miscellaneous/in-context-learning/","title":"In-context Learning (ICL)","text":""},{"location":"Notes/Miscellaneous/in-context-learning/#1-definition","title":"1. Definition","text":"<p>In-context Learning or ICL was defined in \"Language Models are few-shot learners\" by Brown et al., the paper that introduced GPT-3. The authors define ICL as:</p> <p>During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \u201cin-context learning\u201d to describe the inner loop of this process, which occurs within the forward-pass upon each sequence.</p>"},{"location":"Notes/Miscellaneous/in-context-learning/#2-icl-vs-size-of-the-model","title":"2. ICL vs Size of the Model","text":"<p>Aky\u00fcrek et al. make another observation that ICL exhibits algorithmic phase transitions as model depth increases:</p> <ul> <li>One-layer transformers\u2019 ICL behavior approximates a single step of gradient descent, while wider and deeper transformers match ordinary least squares or ridge regression solutions.</li> <li>It is possible to imagine that if small models implement simple learning algorithms in-context, larger models might implement more sophisticated functions during ICL.</li> <li>Smaller models do not seem to learn from in-context examples, and larger ones do.</li> </ul>"},{"location":"Notes/Miscellaneous/in-context-learning/#3-related-discussion","title":"3. Related Discussion","text":"<ol> <li>In ICL over Graphs : PRODIGY, I covered the paper \"PRODIGY : Enabling in-context learning over graphs\" which extends the concept of ICL to graphs.</li> </ol>"},{"location":"Notes/Miscellaneous/types-of-learning/","title":"Types of Learning","text":"<p>There are broadly 3 different types of learning in Machine Learning paradigm:</p> <ul> <li>Supervised Learning</li> <li>Unsupervised Learning</li> <li>Semi-supervised Learning</li> </ul>"},{"location":"Notes/Miscellaneous/types-of-learning/#1-supervised-learning","title":"1. Supervised Learning","text":"<p>When we have lots of labeled samples and we want to train our model to learn from these samples, we use Supervised Learning. Model's performance is measure against a test set which is not seen during training. (We have true labels of the test set)</p>"},{"location":"Notes/Miscellaneous/types-of-learning/#2-unsupervised-learning","title":"2. Unsupervised Learning","text":"<p>When we don't have any labeled data and instead model tries to discover patterns and insights without any explicit guidance or instruction. For example, clustering is an unsupervised learning task.</p>"},{"location":"Notes/Miscellaneous/types-of-learning/#21-self-supervised-learning","title":"2.1. Self-supervised Learning","text":"<ul> <li>This is kinda unsupervised but with an additional supervisory signal.</li> <li>For example, one perform augmentations on the same images and label them as a positive pair, different images as negative pair, and attempts to push the learnt features of negative pairs away while dragging positive features close.</li> <li>Constrastive learning is a type of self-supervised learning.</li> <li>This enables the network to learn to group images of similar classes, which allows model to perform classification / segmentation tasks without having ground truths.</li> </ul>"},{"location":"Notes/Miscellaneous/types-of-learning/#3-semi-supervised-learning","title":"3. Semi-supervised Learning","text":"<p>When we have a few labeled samples and a lot of unlabeled samples, we want to be able to use both of them to optimize the performance and learning capability of our model. This is the definition of Semi-supervised Learning.</p>"},{"location":"Techie_Tech/","title":"Techie Tech","text":""},{"location":"Techie_Tech/#1-overview","title":"1. Overview","text":"<p>In case it's not clear, Techie Tech is a wordplay on Richie Rich.</p> <p>This section will contain notes on various technologies, tools, and tricks which were helpful for me and in case I need to refer to them again, I can just come here and look them up ;)</p>"},{"location":"Techie_Tech/#2-structure","title":"2. Structure","text":"<p>This section will only contain notes and no sub sections. The notes will be tagged with the technology, tool, or trick they are related to.</p> <p>Alternative Name for this section: Random in Fun as it will contain random things which I found fun to learn.  </p>"},{"location":"Techie_Tech/markdown-things/","title":"Some Markdown Things","text":"<p>These things are specific to this repo, i.e., markdown using <code>mkdocs-material</code>. There are additional changes in <code>extra.css</code> which enables these features. NOTE : These may not work with other markdown editors.</p>","tags":["Markdown","LaTeX"]},{"location":"Techie_Tech/markdown-things/#1-note-referencing","title":"1. Note Referencing","text":"<ul> <li>While referencing a note whose name is not unique, like <code>index.md</code> use <code>[&lt;name&gt;](&lt;relative path&gt; like ../Definitions/index#subsection)</code></li> <li>In other cases, directly use <code>[[&lt;name_of_file&gt;#subsection | &lt;name&gt;]]</code>.</li> </ul>","tags":["Markdown","LaTeX"]},{"location":"Techie_Tech/markdown-things/#2-image-aligning-and-resizing","title":"2. Image Aligning and Resizing","text":"<ul> <li>Use <code>{: style=\"height:150px;width:150px\"}</code> after the image link. For example, <code>![](){: style=\"height:10cm;width:15cm\"}</code> or <code>![](){: style=\"height:90%;width:90%</code>.</li> <li>For center aligning, use <code>{: .center}</code> after the image link. For example, <code>![](){: .center style=\"height:100%;width:100%\"}</code>.</li> </ul>","tags":["Markdown","LaTeX"]},{"location":"Techie_Tech/markdown-things/#3-spacing-in-math-mode-latex","title":"3. Spacing in Math Mode (LaTeX)","text":"<p>In increasing order of spacing, use </p> <ul> <li>\\: </li> <li>\\;</li> <li>\\</li> <li>\\quad</li> <li>\\qquad</li> </ul>","tags":["Markdown","LaTeX"]},{"location":"Techie_Tech/DSA/bfs-dfs/","title":"BFS &amp; DFS","text":"","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/bfs-dfs/#1-bfs","title":"1. BFS","text":"<pre><code>vector&lt;int&gt; BFS(int V, vector&lt;int&gt; adj[]) {\n    int visited[V] = {0};\n    visited[0] = 1; //start from node 0\n\n    queue&lt;int&gt; q;\n    q.push(0);\n\n    vector&lt;int&gt; bfs_nodes;\n    while (!q.empty()) {\n        int node = q.front();\n        q.pop();\n        bfs.push_back(node);\n\n        for (int nbr : adj[node]) {\n            if (!visited[nbr]) {\n                visited[nbr] = 1;\n                q.push(nbr);\n            }\n        }\n    }\n    return bfs_nodes;\n}\n</code></pre>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/bfs-dfs/#2-dfs","title":"2. DFS","text":"<p><pre><code>void dfs(int node, vector&lt;int&gt;&amp; adj[], int visited[], vector&lt;int&gt;&amp; dfs_nodes) {\n    visited[node] = 1;\n    dfs_nodes.push_back(node);\n\n    for (int nbr : adj[node]) {\n        if (!visited[nbr]) {\n            dfs (nbr, adj, visited, dfs_nodes);\n        }\n    }\n}\n</code></pre> <pre><code>vector&lt;int&gt; DFS(int V, vector&lt;int&gt; adj[]) {\n    int visited[V] = {0};\n\n    vector&lt;int&gt; dfs_nodes;\n\n    dfs(0, adj, visited, dfs_nodes);\n    return dfs_nodes;\n}\n</code></pre></p>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/bfs-dfs/#3-complexity","title":"3. Complexity","text":"<ul> <li>Time Complexity -&gt; \\(O(V + E)\\)</li> <li>Space Complexity -&gt; \\(O(V)\\)</li> </ul>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/dijkstra/","title":"Dijkstra's Algorithm","text":"","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/dijkstra/#1-problem-statement","title":"1. Problem Statement","text":"<p>Given a graph and a source vertex in the graph, find shortest paths from source to all vertices in the given graph.</p> <ul> <li>NOTE : If graph is a DAG, then simply do Topological Sort  and store the vertices in a stack. Then pop the vertices from the stack and update the distances of the adjacent vertices.</li> <li>This is because to calculate shortest path of any node, we need to calculate the shortest path of all the nodes that come before it. Thus, this automatically calls for topological sorting.</li> </ul>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/dijkstra/#2-solution","title":"2. Solution","text":"<ul> <li>NOTE: Dijkstra\u2019s Algorithm is not valid for negative weights or negative cycles.</li> <li>A negative cycle is a cycle whose edges are such that the sum of their weights is a negative value.</li> <li>Difference between using <code>sets</code> and <code>priority_queues</code> is that in <code>sets</code> we can check if there exists a pair with the same node but a greater distance than the current distance.</li> <li>This is not possible in <code>priority_queues</code> as we cannot erase a particular element from the queue. Thus, we need to push the same node with a smaller distance and the <code>priority_queue</code> will automatically sort it.</li> <li>We can use <code>queues</code> also but it will be slower than the above two methods. </li> </ul>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/dijkstra/#21-using-sets","title":"2.1 Using Sets","text":"<pre><code>vector &lt;int&gt; dijkstra(int V, vector&lt;vector&lt;int&gt;&gt; adj[], int src) {\n    // Create a set for storing the nodes as a pair {dist,node}\n    set&lt;pair&lt;int,int&gt;&gt; st; \n\n    // Initialising dist list with a large number to\n    // indicate the nodes are unvisited initially.\n    // This list contains distance from source to the nodes.\n    vector&lt;int&gt; dist(V, 1e9); \n\n    st.insert({0, src}); \n\n    // Source initialised with dist=0\n    dist[src] = 0;\n\n    // Now, erase the minimum distance node first from the set\n    // and traverse for all its adjacent nodes.\n    while(!st.empty()) {\n        auto it = *(st.begin()); \n\n        int node = it.second; \n        int dis = it.first; \n        st.erase(it); \n\n        // Check for all adjacent nodes of the erased\n        // element whether the prev dist is larger than current or not.\n        for(auto it : adj[node]) {\n            int adjNode = it[0]; \n            int edgW = it[1]; \n\n            if(dis + edgW &lt; dist[adjNode]) {\n                // erase if it was visited previously at \n                // a greater cost.\n                if(dist[adjNode] != 1e9) \n                    st.erase({dist[adjNode], adjNode}); \n\n                // If current distance is smaller,\n                // push it into the queue\n                dist[adjNode] = dis + edgW; \n                st.insert({dist[adjNode], adjNode}); \n            }\n        }\n    }\n    return dist; \n}\n</code></pre>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/dijkstra/#22-using-priority-queues","title":"2.2 Using Priority Queues","text":"<pre><code>vector&lt;int&gt; dijkstra(int V, vector&lt;vector&lt;int&gt;&gt; adj[], int S) {\n    // Create a p.q. for storing the nodes as a pair {dist,node} \n    priority_queue&lt;pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt;&gt;, greater&lt;pair&lt;int, int&gt;&gt;&gt; pq;\n\n    // This list contains distance from source to the nodes.\n    vector&lt;int&gt; distTo(V, INT_MAX);\n\n    // Source initialised with dist=0.\n    distTo[S] = 0;\n    pq.push({0, S});\n\n    // Now, pop the minimum distance node first from the min-heap\n    // and traverse for all its adjacent nodes.\n    while (!pq.empty()) {\n        int node = pq.top().second;\n        int dis = pq.top().first;\n        pq.pop();\n\n        // Check for all adjacent nodes of the popped out\n        // element whether the prev dist is larger than current or not.\n        for (auto it : adj[node]) {\n            int v = it[0];\n            int w = it[1];\n\n            if (dis + w &lt; distTo[v]) {\n                distTo[v] = dis + w;\n\n                // If current distance is smaller,\n                // push it into the queue.\n                pq.push({dis + w, v});\n            }\n        }\n    }\n    return distTo;\n}\n</code></pre>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/dijkstra/#3-complexity","title":"3. Complexity","text":"<ul> <li>Time Complexity --&gt; \\(O(E*logV)\\)</li> <li>Space Complexity --&gt; \\(O(V)\\)</li> </ul>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/lis-binary-search/","title":"Longest Increasing Subsequence (LIS) | Binary Search","text":"","tags":["Algorithms","Dynamic Programming"]},{"location":"Techie_Tech/DSA/lis-binary-search/#1-problem-statement","title":"1. Problem Statement","text":"<p>Given an unsorted array of integers, find the length of longest increasing subsequence.</p>","tags":["Algorithms","Dynamic Programming"]},{"location":"Techie_Tech/DSA/lis-binary-search/#2-solution","title":"2. Solution","text":"<ul> <li>Using DP in \\(O(n^2)\\) time</li> <li> <p>To do this problem in \\(O(n*logn)\\) time, following algo is used :</p> <ul> <li>Initialize a <code>temp</code> array.</li> <li>Push the first element of the <code>arr</code> to <code>temp</code>.</li> <li>Iterate over the next elements.</li> <li>In every iteration, if <code>arr[i]</code> is greater than the last element of the <code>temp</code> array simply push it to the <code>temp</code> array.</li> <li>Else, just find the <code>lower_bound</code> index of that element in the <code>temp</code> array (say <code>ind</code>). Then simply initialize <code>temp[ind] = arr[i]</code>.</li> <li>Maintain a <code>len</code> variable to calculate the length of the <code>temp</code> array in the iteration itself.</li> </ul> </li> </ul> <pre><code>int longestIncreasingSubsequence(int arr[], int n){\n\n    vector&lt;int&gt; temp;\n    temp.push_back(arr[0]);\n\n    int len = 1;\n\n    for(int i=1; i&lt;n; i++){\n        if(arr[i]&gt;temp.back()){\n           // arr[i] &gt; the last element of temp array \n\n           temp.push_back(arr[i]);\n           len++;\n\n        } \n        else{\n    // replacement step\n            int ind = lower_bound(temp.begin(),temp.end(),arr[i]) - temp.begin();\n            temp[ind] = arr[i];\n        }\n\n    }\n\n    return len;\n}\n</code></pre>","tags":["Algorithms","Dynamic Programming"]},{"location":"Techie_Tech/DSA/lis-binary-search/#3-references","title":"3. References","text":"<ol> <li>Strivers DSA sheet | DP-43 | LIS-Binary Search</li> </ol>","tags":["Algorithms","Dynamic Programming"]},{"location":"Techie_Tech/DSA/mcm-dp/","title":"Matrix Chain Multiplication (MCM) | DP","text":"","tags":["Algorithms","Dynamic Programming"]},{"location":"Techie_Tech/DSA/mcm-dp/#1-problem-statement","title":"1. Problem Statement","text":"<p>Given a sequence of matrices, find the most efficient way to multiply these matrices together. The problem is not actually to perform the multiplications, but merely to decide in which order to perform the multiplications.</p> <ul> <li>Twisted version of this problem is given in Leetcode | Minimum score triangulation of Polygon</li> </ul>","tags":["Algorithms","Dynamic Programming"]},{"location":"Techie_Tech/DSA/mcm-dp/#2-solution","title":"2. Solution","text":"<ul> <li>Time Complexity --&gt; \\(O(n^3)\\)</li> <li>Memoized with DP</li> </ul> <pre><code>int f(vector&lt;int&gt;&amp; arr, int i, int j){\n\n    // base condition\n    if(i == j)\n        return 0;\n\n    int mini = INT_MAX;\n\n    // partitioning loop\n    for(int k = i; k&lt;= j-1; k++){\n\n        int ans = f(arr,i,k) + f(arr, k+1,j) + arr[i-1]*arr[k]*arr[j];\n        mini = min(mini,ans);\n\n    }\n    return mini;\n}\n\n\nint matrixMultiplication(vector&lt;int&gt;&amp; arr, int N){\n\n    int i =1;\n    int j = N-1;\n\n    return f(arr,i,j);\n\n\n}\n</code></pre> <ul> <li>Tabulation Method</li> </ul> <pre><code>int matrixMultiplication(vector&lt;int&gt;&amp; arr, int N){\n    vector&lt;vector&lt;int&gt;&gt; dp(N,vector&lt;int&gt;(N,-1));\n\n    for(int i=1; i&lt;N; i++){\n        dp[i][i] = 0;\n    }\n\n    for(int i = N-1; i &gt;= 1; i--){\n\n        for(int j = i+1; j &lt; N; j++){\n\n            int mini = INT_MAX;\n\n            // partioning loop\n            for(int k = i; k&lt;= j-1; k++){\n\n                int ans = dp[i][k]+ dp[k+1][j] + arr[i-1]*arr[k]*arr[j];\n                mini = min(mini,ans);\n\n            }\n\n            dp[i][j] = mini;\n        }\n    }\n    return dp[1][N-1];\n}\n</code></pre> <ul> <li>For the memoized solution we see that for calculating <code>f(i, j)</code> we need to calculate <code>f(i, k)</code> and <code>f(k+1, j)</code> for all <code>i &lt;= k &lt; j</code>. That is for <code>f(i, k)</code> part, we need <code>f(i, i + 1)</code>, <code>f(i, i + 2)</code>, ..., <code>f(i, j - 1)</code>. We see that all these values lie in row <code>i</code> of the memoization table. Similarly, for <code>f(k + 1, j)</code> part, we need <code>f(i + 1, j)</code>, <code>f(i + 2, j)</code>, ..., <code>f(j - 1, j)</code>. We see that all these values lie in column <code>j</code> of the memoization table. </li> <li>Specifically, all the entries that is below <code>f(i, j)</code> but above <code>f(j, j)</code> and to left of <code>f(i, j)</code> but just right of <code>f(i, i)</code> are the entries that we need to calculate <code>f(i, j)</code>.</li> <li>Thus, we first iterate <code>i</code> from <code>N- 1</code> to <code>1</code> so that entries below any row are filled and then iterate <code>j</code> from <code>i + 1</code> to <code>N - 1</code> so that entries to the left of any column are filled.</li> </ul>","tags":["Algorithms","Dynamic Programming"]},{"location":"Techie_Tech/DSA/meet-in-the-middle/","title":"Meet in the Middle","text":"","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/meet-in-the-middle/#1-what-is-this","title":"1.  What is this?","text":"<p>Meet in the middle is a search technique which is used when the input is small but not as small that brute force can be used. Like divide and conquer it splits the problem into two, solves them individually and then merge them. But we can\u2019t apply meet in the middle like divide and conquer because we don\u2019t have the same structure as the original problem.</p> <p>For subset problems, reduce time complexity from \\(O(2^n)\\) to \\(O(2^{n/2}*n)\\).</p>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/meet-in-the-middle/#2-how-to-use-it","title":"2.  How to use it?","text":"<ul> <li> <p>Problem statement :</p> <p>You are given an integer array <code>nums</code> of <code>2 * n</code> integers. You need to partition nums into two arrays of length <code>n</code> to minimize the absolute difference of the sums of the arrays. To partition <code>nums</code>, put each element of <code>nums</code> into one of the two arrays.</p> <p>Return the minimum possible absolute difference.</p> </li> <li> <p>Solution :</p> <ul> <li>Partition the array into two halves of size <code>n / 2</code> each.</li> <li>Pre-compute the sum of all possible subsets / subsequences corresponding to length <code>i</code> in <code>sum1[i]</code> for left half. <code>i</code> varies from <code>0</code> to <code>n / 2</code>. Similarly for the right half.</li> <li>Now to make subset of size <code>n / 2</code>, we take subset of length <code>i</code> from left size and <code>n / 2 - i</code> from right side.</li> <li>Thus, we iterate over <code>i = 0 to i = n / 2</code> and for each <code>sum1[i]</code>, we first sort the <code>sum2[n / 2 - i]</code> and use lower bound to find the closest value to <code>totSum / 2</code>. (<code>totSum</code> is sum of all the elements of the array)</li> <li>Time complexity is \\(O(2 * 2 ^ {n / 2})\\) for finding every subset sum and \\(O(log (2^{n / 2})) = O(n / 2)\\) for binary search. Thus, total time complexity is \\(O(2^{n / 2} * n)\\).</li> </ul> </li> </ul>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/meet-in-the-middle/#3-references","title":"3.  References","text":"<ul> <li>Meet in the Middle - GeeksforGeeks</li> <li>Partition Array Into Two Arrays to Minimize Sum Difference - Leetcode</li> </ul>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sliding-windows/","title":"Sliding Window","text":"","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sliding-windows/#1-type-1","title":"1. Type 1","text":"<p>For problems in which we can update sliding window based on the conditions in the problem, we maintain two pointers <code>i</code> (or start) and <code>j</code> (or end). Run a <code>for</code> loop on <code>j</code>. WHen particular condition is satisfied, update the <code>i</code> pointer and the sliding window and add to the <code>ans</code> accordingly.</p>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sliding-windows/#2-type-2","title":"2. Type 2","text":"<p>Example problem statement : Given an integer array <code>nums</code> and an integer <code>k</code>, return the number of good subarrays of <code>nums</code>. A good array is an array where the number of different integers in that array is exactly <code>k</code>.</p> <p>Solution : Since, updating sliding window based on this rule seems improbable, we will slightly reformulate the problem as finding subarrays with at most <code>k</code> different elements. This way we can find answer simply by <code>atMost(k) - atMost(k - 1)</code>.</p> <pre><code>int subarraysWithKDistinct(vector&lt;int&gt;&amp; nums, int k) {\n        return atMost(nums, k) - atMost(nums, k - 1);\n}\n\nint atMost(vector&lt;int&gt;&amp; nums, int k) {\n    int n = nums.size();\n    int i = 0, j = 0, ans = 0;\n\n    map&lt;int, int&gt; mp;\n\n    for (int j = 0; j &lt; n; j++) {\n        mp[nums[j]]++;\n\n        while (mp.size() &gt; k) {\n            if (mp[nums[i]] == 1) mp.erase(nums[i]);\n            else mp[nums[i]]--;\n            i++;\n        }\n\n        ans += j - i + 1;\n    }\n    return ans;\n}\n</code></pre>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sorting-methods/","title":"Sorting Methods","text":"","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sorting-methods/#1-selection-sort","title":"1. Selection Sort","text":"<pre><code>void selectionSort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int minIndex = i;\n        for (int j = i + 1; j &lt; n; j++) {\n            if (arr[j] &lt; arr[minIndex]) {\n                minIndex = j;\n            }\n        }\n        swap(arr[i], arr[minIndex]);\n    }\n}\n</code></pre> <ul> <li>Time Complexity -&gt; \\(O(n^2)\\)</li> <li>Space Complexity -&gt; \\(O(1)\\)</li> </ul>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sorting-methods/#2-bubble-sort","title":"2. Bubble Sort","text":"<pre><code>void bubbleSort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        for (int j = 0; j &lt; n - i - 1; j++) {\n            if (arr[j] &gt; arr[j + 1]) {\n                swap(arr[j], arr[j + 1]);\n            }\n        }\n    }\n}\n</code></pre> <ul> <li>Time Complexity -&gt; \\(O(n^2)\\)</li> <li>Space Complexity -&gt; \\(O(1)\\)</li> </ul>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sorting-methods/#3-insertion-sort","title":"3. Insertion Sort","text":"<pre><code>void insertionSort(int arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = arr[i];\n        int j = i - 1;\n        while (j &gt;= 0 &amp;&amp; arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n</code></pre> <ul> <li>Time Complexity -&gt; \\(O(n^2)\\)</li> <li>Space Complexity -&gt; \\(O(1)\\)</li> </ul>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sorting-methods/#4-merge-sort","title":"4. Merge Sort","text":"<pre><code>void merge(vector&lt;int&gt; &amp;arr, int low, int mid, int high) {\n    vector&lt;int&gt; temp; \n    int left = low;      \n    int right = mid + 1;  \n\n    while (left &lt;= mid &amp;&amp; right &lt;= high) {\n        if (arr[left] &lt;= arr[right]) {\n            temp.push_back(arr[left]);\n            left++;\n        }\n        else {\n            temp.push_back(arr[right]);\n            right++;\n        }\n    }\n\n    while (left &lt;= mid) {\n        temp.push_back(arr[left]);\n        left++;\n    }\n\n    while (right &lt;= high) {\n        temp.push_back(arr[right]);\n        right++;\n    }\n\n    for (int i = low; i &lt;= high; i++) {\n        arr[i] = temp[i - low];\n    }\n}\n</code></pre> <pre><code>void mergeSort(vector&lt;int&gt; &amp;arr, int low, int high) {\n    if (low &gt;= high) return;\n    int mid = (low + high) / 2 ;\n    mergeSort(arr, low, mid);  \n    mergeSort(arr, mid + 1, high); \n    merge(arr, low, mid, high); \n}\n</code></pre> <ul> <li>Time Complexity -&gt; \\(O(n*logn)\\) </li> <li>Space Complexity -&gt; \\(O(n)\\)</li> <li>Can be done in O(1) space complexity</li> </ul>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/sorting-methods/#5-quick-sort","title":"5. Quick Sort","text":"<p><pre><code>int partition(vector&lt;int&gt; &amp;arr, int low, int high) {\n    int pivot = arr[low];\n    int i = low;\n    int j = high;\n\n    while (i &lt; j) {\n        while (arr[i] &lt;= pivot &amp;&amp; i &lt;= high - 1) {\n            i++;\n        }\n\n        while (arr[j] &gt; pivot &amp;&amp; j &gt;= low + 1) {\n            j--;\n        }\n        if (i &lt; j) swap(arr[i], arr[j]);\n    }\n    swap(arr[low], arr[j]);\n    return j;\n}\n</code></pre> <pre><code>void qs(vector&lt;int&gt; &amp;arr, int low, int high) {\n    if (low &lt; high) {\n        int pIndex = partition(arr, low, high);\n        qs(arr, low, pIndex - 1);\n        qs(arr, pIndex + 1, high);\n    }\n}\n</code></pre> <pre><code>vector&lt;int&gt; quickSort(vector&lt;int&gt; arr) {\n    qs(arr, 0, arr.size() - 1);\n    return arr;\n}\n</code></pre></p> <ul> <li>Avg. Time Complexity -&gt; \\(O(n*logn)\\)</li> <li>Worst Time Complexity -&gt; \\(O(n^2)\\) </li> <li>Space Complexity -&gt; \\(O(1)\\)</li> <li>Auxiliary Space Complexity -&gt; \\(O(n)\\) (for recursive calls on stack) (on avg., it will be \\(O(logn)\\) but in worst case it will be \\(O(n)\\))</li> </ul>","tags":["Algorithms"]},{"location":"Techie_Tech/DSA/stacks-area-of-histogram/","title":"Max area Histogram","text":"","tags":["Algorithms","Stacks"]},{"location":"Techie_Tech/DSA/stacks-area-of-histogram/#1-problem-statement","title":"1. Problem Statement","text":"<p>Given an array of integers heights representing the histogram\u2019s bar height where the width of each bar is 1 return the area of the largest rectangle in histogram.</p>","tags":["Algorithms","Stacks"]},{"location":"Techie_Tech/DSA/stacks-area-of-histogram/#2-solution","title":"2. Solution","text":"","tags":["Algorithms","Stacks"]},{"location":"Techie_Tech/DSA/stacks-area-of-histogram/#21-brute-force","title":"2.1 Brute Force","text":"<pre><code>int largestarea(int arr[], int n) {\n  int maxArea = 0;\n\n  for (int i = 0; i &lt; n; i++) {\n\n    int minHeight = INT_MAX;\n\n    for (int j = i; j &lt; n; j++) {\n      minHeight = min(minHeight, arr[j]);\n      maxArea = max(maxArea, minHeight * (j - i + 1));\n    }\n\n  }\n  return maxArea;\n}\n</code></pre> <ul> <li>Time complexity is \\(O(n^2)\\)</li> </ul>","tags":["Algorithms","Stacks"]},{"location":"Techie_Tech/DSA/stacks-area-of-histogram/#22-using-stack-to-find-the-next-smaller-element","title":"2.2 Using stack to find the next smaller element","text":"<ul> <li>This will be a two pass algorithm and time complexity will be \\(O(n)\\)</li> </ul>","tags":["Algorithms","Stacks"]},{"location":"Techie_Tech/DSA/topo-sort/","title":"Topological Sort","text":"","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/topo-sort/#1-definition","title":"1. Definition","text":"<ul> <li>In topological sorting, node <code>u</code> will always appear before node <code>v</code> if there is a directed edge from node <code>u</code> towards node <code>v</code>, i.e., <code>u</code> -&gt; <code>v</code>.</li> <li>This is only possible for Directed Acyclic Graphs (DAGs).</li> </ul>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/topo-sort/#using-dfs","title":"Using DFS","text":"<pre><code>void dfs(int node, int vis[], stack&lt;int&gt; &amp;st, vector&lt;int&gt; adj[]) {\n    vis[node] = 1;\n    for (auto it : adj[node]) {\n        if (!vis[it]) dfs(it, vis, st, adj);\n    }\n    st.push(node);\n}\n\nvector&lt;int&gt; topoSort(int V, vector&lt;int&gt; adj[]) {\n    int vis[V] = {0};\n    stack&lt;int&gt; st;\n\n    for (int i = 0; i &lt; V; i++) {\n        if (!vis[i]) {\n            dfs(i, vis, st, adj);\n        }\n    }\n\n    vector&lt;int&gt; ans;\n    while (!st.empty()) {\n        ans.push_back(st.top());\n        st.pop();\n    }\n    return ans;\n}\n</code></pre> <ul> <li>This Algorithm assumes that graph is DAG.</li> </ul>","tags":["Algorithms","Graphs"]},{"location":"Techie_Tech/DSA/topo-sort/#using-bfs-kahns-algorithm","title":"Using BFS (Kahn's Algorithm)","text":"<pre><code>vector&lt;int&gt; findOrder(int n, vector&lt;vector&lt;int&gt;&gt;&amp; adj) {\n    vector&lt;int&gt; indegree(n, 0);\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int j : adj[i]) {\n            indegree[j]++;\n        }\n    }\n\n    queue&lt;int&gt; q;\n    for (int i = 0; i &lt; n; i++) {\n        if (indegree[i] == 0) q.push(i);\n    }\n\n    vector&lt;int&gt; ans;\n    while (!q.empty()) {\n        int node = q.front();\n        q.pop();\n        ans.push_back(node);\n\n        for (int nbr : adj[node]) {\n            if (--indegree[nbr] == 0) {\n                q.push(nbr);\n            }\n        }\n    }\n\n    return (ans.size() == n) ? ans : vector&lt;int&gt;(); //check if there's a cycle\n}\n</code></pre>","tags":["Algorithms","Graphs"]}]}