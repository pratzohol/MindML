{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"Notes/Graph_Neural_Networks/icl-over-graphs/","title":"ICL over Graphs","text":"<p>In this note, I will cover the following paper \"PRODIGY : Enabling in-context learning over graphs\". </p>"},{"location":"Notes/Graph_Neural_Networks/icl-over-graphs/#introduction","title":"Introduction","text":"<p>We can easily infer from In-context Learning (ICL) that the in-context learning is a novel and one of the most intriguing capabilities of language models. However, how to enable in-context learning over graphs is still an unexplored question.</p> <p>An in-context learner for graphs should be able to solve novel tasks on novel graphs. For example, give music product recommendations on Spotify when being trained on Amazon purchasing graph. </p>"},{"location":"Notes/Graph_Neural_Networks/icl-over-graphs/#challenges","title":"Challenges","text":"<ol> <li>How to formulate and represent node-, edge- and graph-level tasks over graphs with a unified task representation that allows the model to solve diverse tasks without the need for retraining or parameter tuning.</li> <li>How to design model architecture and pre-training objectives that enable in-context learning capabilities across diverse tasks and diverse graphs in the unified task representation.</li> <li>Existing graph pre-training methods only aim to learn good graph encoder and require fine-tuning to adapt to different tasks, while existing meta-learning methods over graphs only aim to generalize across tasks within same graph.</li> </ol> <pre><code>Achieving in-context learning requires generalizing across different graphs and\ntasks without fine-tuning or parameter tuning.\n</code></pre>"},{"location":"Notes/Miscellaneous/in-context-learning/","title":"In-context Learning (ICL)","text":""},{"location":"Notes/Miscellaneous/in-context-learning/#definition","title":"Definition","text":"<p>In-context Learning or ICL was defined in \"Language Models are few-shot learners\" by Brown et al., the paper that introduced GPT-3. The authors define ICL as:</p> <p>During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \u201cin-context learning\u201d to describe the inner loop of this process, which occurs within the forward-pass upon each sequence.</p>"},{"location":"Notes/Miscellaneous/in-context-learning/#icl-vs-size-of-the-model","title":"ICL vs Size of the Model","text":"<p>Aky\u00fcrek et al. make another observation that ICL exhibits algorithmic phase transitions as model depth increases:</p> <ul> <li>One-layer transformers\u2019 ICL behavior approximates a single step of gradient descent, while wider and deeper transformers match ordinary least squares or ridge regression solutions.</li> <li>It is possible to imagine that if small models implement simple learning algorithms in-context, larger models might implement more sophisticated functions during ICL.</li> <li>Smaller models do not seem to learn from in-context examples, and larger ones do.</li> </ul>"},{"location":"Notes/Miscellaneous/in-context-learning/#related-discussion","title":"Related Discussion","text":"<ol> <li>In ICL over Graphs, I covered the paper \"PRODIGY : Enabling in-context learning over graphs\" which extends the concept of ICL to graphs.</li> </ol>"}]}