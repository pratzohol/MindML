{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"Notes/Miscellaneous/In-context%20Learning/","title":"In-Context Learning (ICL)","text":""},{"location":"Notes/Miscellaneous/In-context%20Learning/#definition","title":"Definition","text":"<p>In-context Learning or ICL was defined in \"Language Models are few-shot learners\" by Brown et al., the paper that introduced GPT-3. The authors define ICL as:</p> <p>During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \u201cin-context learning\u201d to describe the inner loop of this process, which occurs within the forward-pass upon each sequence.</p>"},{"location":"Notes/Miscellaneous/In-context%20Learning/#icl-vs-size-of-the-model","title":"ICL vs Size of the Model","text":"<p>Aky\u00fcrek et al. make another observation that ICL exhibits algorithmic phase transitions as model depth increases:</p> <ul> <li>one-layer transformers\u2019 ICL behavior approximates a single step of gradient descent, while wider and deeper transformers match ordinary least squares or ridge regression solutions.</li> <li>It is possible to imagine that if small models implement simple learning algorithms in-context, larger models might implement more sophisticated functions during ICL.</li> <li>Smaller models do not seem to learn from in-context examples, and larger ones do.</li> </ul>"}]}