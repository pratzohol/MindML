{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>I'm a Fourth year Undergraduate at IIT Delhi majoring in Electrical Engineering. My core interests are in Machine Learning, Deep Learning, and Artificial Intelligence. In addition to these, I enjoy new technology, problem-solving and playing outdoor sports.</p>"},{"location":"#what-is-mindml","title":"What is MindML?","text":"<p>Inspired from Notes on AI, MindML is a personal wiki on AI and ML. It is a collection of notes, resources, and ideas on various topics in AI and ML. The search function and simplistic site makes it easy to navigate and revisit the concepts I have learnt.</p>"},{"location":"Notes/Graph_Neural_Networks/icl-over-graphs/","title":"ICL over Graphs","text":"<p>In this note, I will cover the following paper \"PRODIGY : Enabling in-context learning over graphs\".</p> <p>NOTE : Definition of in-context learning (ICL) is covered in this post (In-context Learning (ICL)).</p>"},{"location":"Notes/Graph_Neural_Networks/icl-over-graphs/#introduction","title":"Introduction","text":"<p>We can easily infer that the in-context learning is a novel and one of the most intriguing capabilities of language models. However, how to enable in-context learning over graphs is still an unexplored question.</p> <p>An in-context learner for graphs should be able to solve novel tasks on novel graphs. For example, give music product recommendations on Spotify when being trained on Amazon purchasing graph. </p>"},{"location":"Notes/Graph_Neural_Networks/icl-over-graphs/#challenges","title":"Challenges","text":"<ol> <li>How to formulate and represent node-, edge- and graph-level tasks over graphs with a unified task representation that allows the model to solve diverse tasks without the need for retraining or parameter tuning.</li> <li>How to design model architecture and pre-training objectives that enable in-context learning capabilities across diverse tasks and diverse graphs in the unified task representation.</li> <li>Existing graph pre-training methods only aim to learn good graph encoder and require fine-tuning to adapt to different tasks, while existing meta-learning methods over graphs only aim to generalize across tasks within same graph.</li> </ol> <pre><code>Achieving in-context learning requires generalizing across different graphs and\ntasks without fine-tuning or parameter tuning.\n</code></pre>"},{"location":"Notes/Graph_Neural_Networks/icl-over-graphs/#_1","title":"ICL over Graphs","text":""},{"location":"Notes/Miscellaneous/contrastive-learning/","title":"Contrastive Learning","text":""},{"location":"Notes/Miscellaneous/contrastive-learning/#definition","title":"Definition","text":"<p>Contrastive learning aims at learning low-dimensional representations of data by contrasting between similar and dissimilar samples.</p> <p>What this means is that it tries to bring similar samples close to each other in the representation space and push dissimilar samples away from each other.</p> <p>Let's suppose we have 3 images, \\(I_1\\), \\(I_2\\) and \\(I_3\\), where \\(I_1\\) and \\(I_2\\) belongs to same class (e.x. dog) and \\(I_3\\) belongs to different class (e.x. cat). The representation space will look something like this:</p> <p></p> <p>We see that the distance \\(d(x_1, x_2)\\) is small compared to \\(d(x_1, x_3)\\) and \\(d(x_2, x_3)\\) where \\(d()\\) is a metric function like euclidean.</p>"},{"location":"Notes/Miscellaneous/contrastive-learning/#loss-functions","title":"Loss Functions","text":""},{"location":"Notes/Miscellaneous/contrastive-learning/#contrastive-loss","title":"Contrastive Loss","text":"<p>We suppose that we have a pair (\\(I_i\\), \\(I_j\\)) and a label \\(Y\\) that is equal to 0 if the samples are similar and 1 otherwise. To extract a low-dimensional representation of each sample, we use a Convolutional Neural Network \\(f\\) that encodes the input images \\(I_i\\) and \\(I_j\\) into an embedding space where \\(x_i = f(I_i)\\) and \\(x_j = f(I_j)\\). The contrastive loss is defined as:  </p>"},{"location":"Notes/Miscellaneous/contrastive-learning/#triplet-loss","title":"Triplet Loss","text":""},{"location":"Notes/Miscellaneous/in-context-learning/","title":"In-context Learning (ICL)","text":""},{"location":"Notes/Miscellaneous/in-context-learning/#definition","title":"Definition","text":"<p>In-context Learning or ICL was defined in \"Language Models are few-shot learners\" by Brown et al., the paper that introduced GPT-3. The authors define ICL as:</p> <p>During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \u201cin-context learning\u201d to describe the inner loop of this process, which occurs within the forward-pass upon each sequence.</p>"},{"location":"Notes/Miscellaneous/in-context-learning/#icl-vs-size-of-the-model","title":"ICL vs Size of the Model","text":"<p>Aky\u00fcrek et al. make another observation that ICL exhibits algorithmic phase transitions as model depth increases:</p> <ul> <li>One-layer transformers\u2019 ICL behavior approximates a single step of gradient descent, while wider and deeper transformers match ordinary least squares or ridge regression solutions.</li> <li>It is possible to imagine that if small models implement simple learning algorithms in-context, larger models might implement more sophisticated functions during ICL.</li> <li>Smaller models do not seem to learn from in-context examples, and larger ones do.</li> </ul>"},{"location":"Notes/Miscellaneous/in-context-learning/#related-discussion","title":"Related Discussion","text":"<ol> <li>In ICL over Graphs, I covered the paper \"PRODIGY : Enabling in-context learning over graphs\" which extends the concept of ICL to graphs.</li> </ol>"},{"location":"Random_is_Fun/markdown-things/","title":"Some Markdown Things","text":""},{"location":"Random_is_Fun/markdown-things/#_1","title":"Some Markdown Things","text":""}]}